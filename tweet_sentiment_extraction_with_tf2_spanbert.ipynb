{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "tweet-sentiment-extraction-with-tf2-spanbert.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r7EgX-bXJSW"
      },
      "source": [
        "Here, we're going to adapt this example from the Keras website (https://keras.io/examples/nlp/text_extraction_with_bert/) to apply it to the Tweet Sentiment Extraction competition. We're going to take the text of a tweet and its sentiment, and return the part of the text that embodies its sentiment (positive, negative, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6928338a-2ea6-4bd8-9d00-9b0015749426",
        "_cell_guid": "3574b925-f9b1-4e21-a1fc-69cfa9f8f2b7",
        "trusted": true,
        "id": "8bsnqfrtXJSb"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9ead4695-fd2c-4c29-8915-753ebe18f1a2",
        "_cell_guid": "547febea-6065-4fee-bcaf-796d80cd4b71",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1SHNrVBXJSe",
        "outputId": "bdd588a5-965a-46a6-ddf0-93b16f560117"
      },
      "source": [
        "!pip install transformers\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig, BertTokenizerFast \n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "KHiJ8eHPaDBw",
        "outputId": "90fc1797-8001-4d7b-8e71-1738cabbed1c"
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-df1416c0-b526-4dad-8db2-5b6ef4dd9b4d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-df1416c0-b526-4dad-8db2-5b6ef4dd9b4d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving tt.csv to tt (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "iKU4PF4x6lBy",
        "outputId": "8d7f708d-9cf7-4ddc-b2e4-a7adf3ed96d0"
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0d82bd96-5a92-4c2a-aafe-568fef11f366\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0d82bd96-5a92-4c2a-aafe-568fef11f366\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving tm.csv to tm (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4969e90d-545f-4068-9f35-5078a38357bb",
        "_cell_guid": "1717ff0e-f88c-49b7-8932-0cf5c809f334",
        "trusted": true,
        "id": "gFjSe5L7XJSf"
      },
      "source": [
        "data = pd.read_csv(\"tt.csv\")\n",
        "test = pd.read_csv(\"tm.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1dypOnEXJSg"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ShUJue_XJSg"
      },
      "source": [
        "I've noticed that for the neutral tweets, most of the time the \"selected text\" is just the whole tweet. So, I'm going to go by that assumption, and only predict the selected text for the positive or negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9d664b28-3dbc-4ba1-a907-49cd6e20ee61",
        "_cell_guid": "fbd579c2-5d14-422b-9689-4393044b7aff",
        "trusted": true,
        "id": "TdsvM0PiXJSh"
      },
      "source": [
        "data = data[data.selected_text != \"no_cause\"]\n",
        "data = data[pd.notnull(data.selected_text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a4f68171-7627-49ed-9813-1da3ac49fcb8",
        "_cell_guid": "8d38c225-4ac5-48dc-ab9a-cb146764308a",
        "trusted": true,
        "id": "TiufVmoRXJSi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "b10d3797-dd2a-4d92-af03-0fc2a51cec7e"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6e0c6d75b1</td>\n",
              "      <td>2am feedings for the baby are fun when he is a...</td>\n",
              "      <td>fun</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... sentiment\n",
              "1  549e992a42  ...  negative\n",
              "2  088c60f138  ...  negative\n",
              "3  9642c003ef  ...  negative\n",
              "4  358bd9e861  ...  negative\n",
              "6  6e0c6d75b1  ...  positive\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfjTbiMgXJSj"
      },
      "source": [
        "We're going to use SpanBERT, a version of BERT specifically trained for predicting spans, as our transformer.\n",
        "\n",
        "For more information about SpanBERT: https://github.com/facebookresearch/SpanBERT\n",
        "\n",
        "For more information about BERT: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
        "\n",
        "To learn about transformers in general: https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04\n",
        "\n",
        "Feel free to explore different transformers- there are many great options!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p8D2nDdXJSk"
      },
      "source": [
        "The first step is to tokenize the text, which turns it into an array of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "bc5a754b-0caf-4561-814f-b72491700ad9",
        "_cell_guid": "289ab6e5-c5b4-4f5f-adbb-4857d5c3619e",
        "trusted": true,
        "id": "pwC0h_wZXJSk"
      },
      "source": [
        "max_len = 384\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sowPryDkXJSl"
      },
      "source": [
        "The example from the Keras website (https://keras.io/examples/nlp/text_extraction_with_bert/ is for the SQUAD dataset, one of the most famous text extraction datasets. In that dataset, there is a \"context\" (e.g. a paragraph), a \"question\", and an \"answer\" (which is a subset of the context). In this case, we can treat the tweet as the context, the sentiment as the question, and the selected text of the tweet as the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y_T50m3XJSl"
      },
      "source": [
        "Below, we create an object for the input data. This tokenizes the text and selected text, finds the indices of the selected text in the text, and prepares the data for input to BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "89cad531-0ea0-46b9-88b2-250a362ed4d9",
        "_cell_guid": "abd61fc3-41e0-4ab4-955a-d364c921cfa1",
        "trusted": true,
        "id": "cZuXPdEeXJSm"
      },
      "source": [
        "class TrainTweetExample:\n",
        "    def __init__(self, sentiment, text, start_char_idx, selected_text, all_answers):\n",
        "        self.sentiment = sentiment\n",
        "        self.text = text\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.selected_text = selected_text\n",
        "        self.all_answers = all_answers\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        text = self.text\n",
        "        sentiment = self.sentiment\n",
        "        selected_text = self.selected_text\n",
        "        start_char_idx = self.start_char_idx\n",
        "\n",
        "        # Clean text, sentiment, and answer\n",
        "        sentiment = \" \".join(str(sentiment).split())\n",
        "        text = \" \".join(str(text).split())\n",
        "        answer = \" \".join(str(selected_text).split())\n",
        "\n",
        "        # Find end character index of answer in text\n",
        "        end_char_idx = start_char_idx + len(answer)\n",
        "        if end_char_idx >= len(text):\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Mark the character indexes in text that are in answer\n",
        "        is_char_in_ans = [0] * len(text)\n",
        "        for idx in range(start_char_idx, end_char_idx):\n",
        "            is_char_in_ans[idx] = 1\n",
        "\n",
        "        # Tokenize text\n",
        "        tokenized_text = tokenizer.encode_plus(text, return_offsets_mapping=True, max_length = max_len)\n",
        "\n",
        "        # Find tokens that were created from answer characters\n",
        "        ans_token_idx = []\n",
        "        for idx, (start, end) in enumerate(tokenized_text.offset_mapping):\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\n",
        "                ans_token_idx.append(idx)\n",
        "\n",
        "        if len(ans_token_idx) == 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Find start and end token index for tokens from answer\n",
        "        start_token_idx = ans_token_idx[0]\n",
        "        end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "        # Tokenize sentiment\n",
        "        tokenized_sentiment = tokenizer.encode_plus(sentiment, return_offsets_mapping=True, max_length = max_len)\n",
        "\n",
        "        # Create inputs\n",
        "        input_ids = tokenized_text.input_ids + tokenized_sentiment.input_ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_text.input_ids) + [1] * len(\n",
        "            tokenized_sentiment.input_ids[1:]\n",
        "        )\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad and create attention masks.\n",
        "        # Skip if truncation is needed\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:  # pad\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:  # skip\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.context_token_to_char = tokenized_text.offset_mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "350d9a50-3a49-45ee-bbe7-52b4fa8675ff",
        "_cell_guid": "facfe2c9-7536-4675-969e-103ffa455038",
        "trusted": true,
        "id": "wZCwB5UaXJSn"
      },
      "source": [
        "def create_tweet_examples(data):\n",
        "    tweet_examples = []\n",
        "    all_answers = data.selected_text.values\n",
        "    count = 0\n",
        "    for index, row in data.iterrows():\n",
        "        sentiment = row['sentiment']\n",
        "        text = row['text']\n",
        "        selected_text = row['selected_text']\n",
        "\n",
        "        try: ## turn the data into TrainTweetExample objects\n",
        "            start_char_idx = row.text.index(row.selected_text.split()[0])\n",
        "            tweet_eg = TrainTweetExample(\n",
        "                sentiment, text, start_char_idx, selected_text, all_answers\n",
        "            )\n",
        "            tweet_eg.preprocess()\n",
        "            tweet_examples.append(tweet_eg)\n",
        "        except: ## keep track of the number that can't be tokenized/processed\n",
        "            count += 1 \n",
        "    print(\"Couldn't process\",count,\"points\")\n",
        "    return tweet_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHHH-9QTXJSn"
      },
      "source": [
        "We do the same thing for the testing data, which only has the text and sentiment of the tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8e55e1c6-3e3b-49f9-b3ba-5c2b34e9cc40",
        "_cell_guid": "3dac1191-1005-485a-8f34-2d3b8d75d720",
        "trusted": true,
        "id": "eBupzQwMXJSo"
      },
      "source": [
        "class TestTweetExample:\n",
        "    def __init__(self, sentiment, text):\n",
        "        self.sentiment = sentiment\n",
        "        self.text = text\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        text = self.text\n",
        "        sentiment = self.sentiment\n",
        "\n",
        "        # Clean text, answer and sentiment\n",
        "        sentiment = \" \".join(str(sentiment).split())\n",
        "        text = \" \".join(str(text).split())\n",
        "\n",
        "        # Tokenize text\n",
        "        tokenized_text = tokenizer.encode_plus(text, return_offsets_mapping=True, max_length = max_len)\n",
        "\n",
        "        # Tokenize sentiment\n",
        "        tokenized_sentiment = tokenizer.encode_plus(sentiment, return_offsets_mapping=True, max_length = max_len)\n",
        "\n",
        "        # Create inputs\n",
        "        input_ids = tokenized_text.input_ids + tokenized_sentiment.input_ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_text.input_ids) + [1] * len(\n",
        "            tokenized_sentiment.input_ids[1:]\n",
        "        )\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad and create attention masks.\n",
        "        # Skip if truncation is needed\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:  # pad\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:  # skip\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.text_token_to_char = tokenized_text.offset_mapping\n",
        "        self.start_token_idx = 0\n",
        "        self.end_token_idx = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4dc3d8c3-877c-4427-be60-a9acf732014d",
        "_cell_guid": "8f6495c6-dbc3-4a74-9873-03acc9a551c6",
        "trusted": true,
        "id": "l6STxQsdXJSo"
      },
      "source": [
        "def create_tweet_examples_test(data):\n",
        "    tweet_examples = []\n",
        "    all_answers = None\n",
        "    count = 0\n",
        "    for index, row in data.iterrows():\n",
        "        sentiment = row['sentiment']\n",
        "        text = row['text']\n",
        "        selected_text = None\n",
        "\n",
        "        try: ## turn the data into TestTweetExample objects\n",
        "            start_char_idx = 0\n",
        "            tweet_eg = TestTweetExample(\n",
        "                sentiment, text\n",
        "            )\n",
        "            tweet_eg.preprocess()\n",
        "            tweet_examples.append(tweet_eg)\n",
        "        except: ## keep track of the number that can't be tokenized/processed\n",
        "            count += 1\n",
        "    print(\"Couldn't process\",count,\"points\")\n",
        "    return tweet_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "69197991-17c2-4886-b734-6d79f9e66082",
        "_cell_guid": "b4001ffc-b148-4280-9e92-de81aa6bbd24",
        "trusted": true,
        "id": "E1B6Cg9jXJSp"
      },
      "source": [
        "def create_inputs_targets(tweet_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in tweet_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeXNWAw7XJSq"
      },
      "source": [
        "Now, we can split the data into training and validation sets, and apply the preprocessing to the training and testing sets. We want to keep track of the number of data points successfully processed, and if there are any that caused errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f8c94e24-3fc6-45aa-ba30-b4ce2a1fdd33",
        "_cell_guid": "5b573924-8308-4746-a5c6-624c47882a80",
        "trusted": true,
        "id": "yp3YD2j8XJSq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, validation = train_test_split(data, test_size = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2957a93c-0783-4fd4-a50e-df746996bf0d",
        "_cell_guid": "424763a8-adde-421b-808d-ef215cf57879",
        "trusted": true,
        "id": "lRyGg0hkXJSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fa1d24-c87a-43d0-ea48-ecdb785f66c7"
      },
      "source": [
        "train_tweet_examples = create_tweet_examples(train)\n",
        "x_train, y_train = create_inputs_targets(train_tweet_examples)\n",
        "print(f\"{len(train_tweet_examples)} training points created.\")\n",
        "\n",
        "eval_tweet_examples = create_tweet_examples(validation)\n",
        "x_eval, y_eval = create_inputs_targets(eval_tweet_examples)\n",
        "print(f\"{len(eval_tweet_examples)} evaluation points created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Couldn't process 9796 points\n",
            "4930 training points created.\n",
            "Couldn't process 1057 points\n",
            "580 evaluation points created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "372214f1-3a3b-4c03-bdaf-471c0950c916",
        "_cell_guid": "d251a3f5-31f7-43a6-8d1b-961e12a456ed",
        "trusted": true,
        "id": "AiaI2rNLXJSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa25472a-a5f6-4f8a-9ea3-3ab6b41b0aa6"
      },
      "source": [
        "test_tweet_examples = create_tweet_examples_test(test)\n",
        "x_test, _ = create_inputs_targets(test_tweet_examples)\n",
        "print(f\"{len(test_tweet_examples)} test points created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Couldn't process 2104 points\n",
            "0 test points created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1PPUhkxXJSt"
      },
      "source": [
        "## Creating & Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyTehUlIXJSt"
      },
      "source": [
        "Now, we can create the model that uses the SpanBERT transformer, and puts other layers on top of it. Shout-out to this notebook (https://www.kaggle.com/al0kharba/tensorflow-roberta-0-712) for helping me put together the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "43155233-d7b7-4738-a2d8-4e44cf98e220",
        "_cell_guid": "eed8dfd4-2ec2-4198-9154-226eb8b77d0a",
        "trusted": true,
        "id": "jj66dEX7XJSu"
      },
      "source": [
        "def create_model():\n",
        "\n",
        "    PATH = 'bert_base_uncased/vocab.txt'\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    \n",
        "## QA Model\n",
        "    input_ids = layers.Input(shape=(max_len ,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding = encoder(\n",
        "         input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        "    )[0]\n",
        "    \n",
        "\n",
        "    start_logits = layers.Dropout(0.3)(embedding)\n",
        "    start_logits = layers.Conv1D(128,2,padding='same')(start_logits)\n",
        "    start_logits = layers.LeakyReLU()(start_logits)\n",
        "    start_logits = layers.Conv1D(64,2,padding='same')(start_logits)\n",
        "    start_logits = layers.Dense(1, name=\"start_logit\")(start_logits)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "    end_logits = layers.Dropout(0.3)(embedding)\n",
        "    end_logits = layers.Conv1D(128,2,padding='same')(end_logits)\n",
        "    end_logits = layers.LeakyReLU()(end_logits)\n",
        "    end_logits = layers.Conv1D(64,2,padding='same')(end_logits)\n",
        "    end_logits = layers.Dense(1, name=\"end_logit\")(end_logits)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[start_probs, end_probs],\n",
        "    )\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m87NlU28XJSu"
      },
      "source": [
        "The competition was restricted to CPU/GPU, but I found that TPUs greatly sped up training time. To use a TPU, just select it in the menu and change the \"use_tpu\" value below to True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "51913e36-4ffd-4215-8054-bfab23878513",
        "_cell_guid": "7a81f34c-2daa-4a59-9fe7-a3c147e69c9e",
        "trusted": true,
        "id": "aBshUv0RXJSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b664b58-4258-4ad4-89e5-d5c4b6efb049"
      },
      "source": [
        "use_tpu = False\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        model = create_model()\n",
        "else:\n",
        "    model = create_model()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_15 (InputLayer)           [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_4 (TFBertModel)   TFBaseModelOutputWit 109482240   input_13[0][0]                   \n",
            "                                                                 input_15[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_193 (Dropout)           (None, 384, 768)     0           tf_bert_model_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_194 (Dropout)           (None, 384, 768)     0           tf_bert_model_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 384, 128)     196736      dropout_193[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 384, 128)     196736      dropout_194[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 384, 128)     0           conv1d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 384, 128)     0           conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 384, 64)      16448       leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 384, 64)      16448       leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "start_logit (Dense)             (None, 384, 1)       65          conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "end_logit (Dense)               (None, 384, 1)       65          conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_8 (Flatten)             (None, 384)          0           start_logit[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 384)          0           flatten_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 384)          0           flatten_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,908,738\n",
            "Trainable params: 109,908,738\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FihDOlkBXJSv"
      },
      "source": [
        "Below is how we're going to evaluate the results of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1b8c5e60-dc48-4d13-a15c-18db1203c014",
        "_cell_guid": "d3c4acde-66b5-4785-b5cb-8afb45fcd81d",
        "trusted": true,
        "id": "7x3TCJLiXJSv"
      },
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    exclude = set(string.punctuation)\n",
        "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    # Remove articles\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # Remove extra white space\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "\n",
        "class ExactMatch(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Each `TweetExample` object contains the character level offsets for each token\n",
        "    in its input text. We use them to get back the span of text corresponding\n",
        "    to the tokens between our predicted start and end tokens.\n",
        "    All the ground-truth answers are also present in each `TweetExample` object.\n",
        "    We calculate the percentage of data points where the span of text obtained\n",
        "    from model predictions matches one of the ground-truth answers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        count = 0\n",
        "        eval_examples_no_skip = [_ for _ in eval_tweet_examples if _.skip == False]\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            tweet_eg = eval_examples_no_skip[idx]\n",
        "            offsets = tweet_eg.context_token_to_char\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if start >= len(offsets):\n",
        "                continue\n",
        "            pred_char_start = offsets[start][0]\n",
        "            if end < len(offsets):\n",
        "                pred_char_end = offsets[end][1]\n",
        "                pred_ans = tweet_eg.text[pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                pred_ans = tweet_eg.text[pred_char_start:]\n",
        "\n",
        "            normalized_pred_ans = normalize_text(pred_ans)\n",
        "            normalized_true_ans = [normalize_text(_) for _ in tweet_eg.all_answers]\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQqj2uY4XJSw"
      },
      "source": [
        "Now, we train the model. I'm only using one epoch, but feel free to update and explore different training setups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "842c1a3b-3b09-4034-a3c8-c5c9d370e270",
        "_cell_guid": "685c1204-0552-47cf-9b45-42325c977411",
        "trusted": true,
        "id": "NfOo9fpbXJSw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "ed59be7e-74ea-4f7a-8ebc-0b904589f8f7"
      },
      "source": [
        "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=1,  # For demonstration, 3 epochs are recommended\n",
        "    verbose=1,\n",
        "    batch_size=64,\n",
        "    callbacks=[exact_match_callback],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-192-dbc9c5a18a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexact_match_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f7889b7-8df6-45f2-b7cc-23a3b03d0920",
        "_cell_guid": "7c173ff3-5346-4bb8-bc0f-7f4eadac7ba3",
        "trusted": true,
        "id": "R6s800qRXJSw"
      },
      "source": [
        "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
        "model.fit(\n",
        "    list(np.array(x)),\n",
        "    list(np.array(y)),\n",
        "    epochs=1,\n",
        "    verbose=1,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[exact_match_callback],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVdOnEtQXJSx"
      },
      "source": [
        "## Generating Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFtvpdTHXJSx"
      },
      "source": [
        "Let's use the model to predict the selected text for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e9ac47e7-0ee7-4e5f-9758-4597de4bd9a5",
        "_cell_guid": "917d3ccb-884f-492c-8db1-016b3c72b91f",
        "trusted": true,
        "id": "Jct7jHExXJSx"
      },
      "source": [
        "pred = model.predict(x_test)\n",
        "np.array(pred).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "af59199a-36c8-4653-a1bd-5d53b38f8364",
        "_cell_guid": "d22e9e6c-e02f-4bfb-9264-8aee69afd8a0",
        "trusted": true,
        "id": "jz1hEYpaXJSx"
      },
      "source": [
        "test['prediction'] = np.zeros(len(test.text.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d8fea9c1-7320-40c2-8d07-ff7c2a0ad214",
        "_cell_guid": "a98d37a3-9b1d-4719-a3ef-78be34e870f3",
        "trusted": true,
        "id": "-aZdqcrIXJSx"
      },
      "source": [
        "pred_start, pred_end = pred\n",
        "count = 0\n",
        "\n",
        "pred_text = []\n",
        "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    tweet_eg = test_tweet_examples[idx]\n",
        "    if (tweet_eg.skip == True):\n",
        "        pred_text.append(\"\")\n",
        "    else: \n",
        "        offsets = tweet_eg.text_token_to_char\n",
        "        start = np.argmax(start)\n",
        "        end = np.argmax(end)\n",
        "        if start >= len(offsets):\n",
        "            continue\n",
        "        pred_char_start = offsets[start][0]\n",
        "        if end < len(offsets):\n",
        "            pred_char_end = offsets[end][1]\n",
        "            pred_ans = tweet_eg.text[pred_char_start:pred_char_end + 1]\n",
        "        else:\n",
        "            pred_ans = tweet_eg.text[pred_char_start:]\n",
        "\n",
        "        normalized_pred_ans = normalize_text(pred_ans)\n",
        "        #print(normalized_pred_ans)\n",
        "        pred_text.append(normalized_pred_ans)\n",
        "        test['prediction'][idx] = normalized_pred_ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gzfUV36XJSy"
      },
      "source": [
        "Now, we can go back and fill in any gaps, like neutral tweets, and remove hyperlinks from the final results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "202fd3c7-3bc5-4bfb-ba2f-c5ab71cbc59f",
        "_cell_guid": "358aa64d-54ca-450c-9200-b41685a7ae2b",
        "trusted": true,
        "id": "DNhfufBGXJSy"
      },
      "source": [
        "test.prediction[test.prediction == 0] = test.text[test.prediction == 0]\n",
        "test.prediction[test.sentiment == \"neutral\"] = test.text[test.sentiment==\"neutral\"]\n",
        "test.prediction[test.prediction ==''] = test.text[test.prediction =='']\n",
        "\n",
        "def clean_text(text):\n",
        "    words = str(text).split()\n",
        "    words = [x for x in words if not x.startswith(\"http\")]\n",
        "    words = \" \".join(words)\n",
        "    return words\n",
        "\n",
        "test['prediction'] = test['prediction'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "49309b43-36ee-40b7-8d8e-9cccc17dfd51",
        "_cell_guid": "10306ef3-9ad5-401d-b52c-878ca03d591e",
        "trusted": true,
        "id": "DbPbS1RUXJSy"
      },
      "source": [
        "test[:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnr5D7dcXJSy"
      },
      "source": [
        "Now, we can just write our submission file!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3477f9ec-6104-42ca-9b88-aad0ed9fd322",
        "_cell_guid": "c8bd192b-9a1b-43af-ac43-86fe943a1907",
        "trusted": true,
        "id": "9wx1_aVMXJSy"
      },
      "source": [
        "evaluation = test.textID.copy().to_frame()\n",
        "evaluation['selected_text'] = test['prediction']\n",
        "evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b56fccc7-1061-463b-a1f3-c8c2c58b4919",
        "_cell_guid": "87572eeb-6315-45a2-9e42-a63b8d33bbe2",
        "trusted": true,
        "id": "7nPcu3Q0XJSy"
      },
      "source": [
        "evaluation.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}